{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46258316",
   "metadata": {},
   "source": [
    "# Natural Language Processing Assignment 2B: The Notebook\n",
    "\n",
    "This is the notebook for the second hand-in assignment for Natural Language Processing. The notebook counts for 65% of the total assignment, the total assignment counts towards 15% of the final grade.\n",
    "\n",
    "In this notebook, you will be working on Sentiment Classification over IMDB movie reviews! As usual, you can progress through the notebook from top to bottom. The assignment broadly consists of three parts:\n",
    "\n",
    "1. Data preparation: where you will learn about the task, and prepare the data to be consumed by your PyTorch model.\n",
    "2. Model training: where you define the appropriate RNN model and training specifics to train the classifier.\n",
    "3. Results analysis: where you will inspect the model's learned word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd823cea",
   "metadata": {},
   "source": [
    "## Part 4 (15 points): Data preparation\n",
    "\n",
    "In this part you will prepare the data from the IMDB 50K dataset, so that it can be processed by a PyTorch LSTM-based recurrent model.\n",
    "\n",
    "First, some usual imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be6cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch import Tensor, FloatTensor, LongTensor\n",
    "from typing import Tuple, List, Callable, Optional\n",
    "# If the stopwords aren't available, we need to download them from NLTK\n",
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "except LookupError:\n",
    "    import nltk\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af9bf79",
   "metadata": {},
   "source": [
    "### The Task: Movie Review Classification\n",
    "\n",
    "The [IMDB 50k dataset](https://ai.stanford.edu/~amaas/data/sentiment/) contains 50000 \"highly polar\" movie reviews, that come together with a label for their sentiment ('positive' or 'negative'). Let's load in the data and inspect the first entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21143ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./IMDB Dataset.csv', newline='') as csvfile:\n",
    "    review_reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    reviews, sentiments = zip(*[r for r in review_reader][1:])\n",
    "reviews = [r for r in reviews]\n",
    "sentiments = list(sentiments)\n",
    "display(reviews[0])\n",
    "display(sentiments[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefb2ec8",
   "metadata": {},
   "source": [
    "### Part 4.1 (2 point):\n",
    "\n",
    "You can see that the data could contain rather long sequences! We will deviate from the exercises in that we will make use of an internal `torch.nn.Embedding` layer to hold and update word embeddings. This layer will hold a single embedding for each word, so for the sake of fast training this layer shouldn't get too big. Therefore, let's figure out how many words our (so far unprocessed) dataset contains. To warm up, finish the below implementation to figure out how many unique words our data contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92efe3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Solution:\n",
    "\n",
    "def count_vocab(texts: List[str]) -> int:\n",
    "    NotImplemented\n",
    "\n",
    "display(count_vocab(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498532c7",
   "metadata": {},
   "source": [
    "### Part 4.2 (5 points):\n",
    "\n",
    "Yikes! That's way too many words, so we will definitely need to do some preprocessing. But honestly, even after preprocessing we will still have too many words to cover in our model, unless your computer has a very large working memory ;-)\n",
    "\n",
    "So our strategy will be twofold: first, we want to preprocess a little bit by lowercasing everything and removing unwanted characters. Then, we want to keep only a fixed number of words for each review. In anticipation of the fact that our model's embedding layer will expect number representations of words, we will have to extract a vocabulary of words that we will represent as integers.\n",
    "\n",
    "In the code below, the first part has already been implemented: the function `preprocess_string` uses regular expressions to remove unwanted characters, `preprocess_text` lowercases a review text and preprocesses each word.\n",
    "\n",
    "The bottom part was also implemented: the `tensorize_sentences` function will take a list of (preprocessed!) reviews, and a dictionary that maps the words in our vocabulary to integer values for our PyTorch model.\n",
    "\n",
    "Your job is to complete the implementation of `tokenize_data`. As you can see from the type information, it expects a list of strings (the original reviews), and outputs a pair: a list of processed reviews and the vocabulary map. The processed reviews are already given, but you need to figure out how to create the `word2int` dictionary mapping. It must adhere to the following specifications:\n",
    "\n",
    "1. It can't contain any stopwords,\n",
    "2. It must contain exactly 5000 words,\n",
    "3. It must map to the numbers 1-5000; the number 0 will be reserved for padding of the dataset later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b259cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Solution:\n",
    "\n",
    "def preprocess_string(s: str) -> str:\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # Replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "    return s\n",
    "\n",
    "def preprocess_text(text: str) -> List[str]:\n",
    "    return [preprocess_string(word) for word in text.lower().split()]\n",
    "\n",
    "def tokenize_data(all_texts: List[str]) -> Tuple[List[List[str]], dict[str, int]]:\n",
    "    processed_reviews = [preprocess_text(s) for s in all_texts]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word2int = NotImplemented\n",
    "    return processed_reviews, word2int\n",
    "\n",
    "def tensorize_sentences(preprocessed_texts: List[List[str]], word_map: dict[str, int]) -> List[Tensor]:\n",
    "    return [torch.tensor([word_map[w] for w in s if w in word_map]) for s in preprocessed_texts]\n",
    "\n",
    "def tensorize_sentiments(sentiments: List[str]) -> Tensor:\n",
    "    return [torch.tensor(1.) if sentiment == 'positive' else torch.tensor(0.) for sentiment in sentiments]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef67cc8",
   "metadata": {},
   "source": [
    "Verify your implementation above by running the code below to preprocess and tokenize everything. Then run the code below that to inspect the result for the first review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_reviews, word2int = tokenize_data(reviews)\n",
    "tokenized_reviews = tensorize_sentences(processed_reviews, word2int)\n",
    "sentiment_tensors = tensorize_sentiments(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b058d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(reviews[0])\n",
    "display(tokenized_reviews[0])\n",
    "display(sentiment_tensors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a76b81",
   "metadata": {},
   "source": [
    "### Part 4.3 (8 points):\n",
    "\n",
    "As usual, we will have to split up our data and wrap it inside `Dataset`s and `DataLoader`s. While the splitting function and the `ReviewDataset` implementation are already given, we still need to implement the padding function, that is going to make sure that all sequences are padded before being passed to the model. \n",
    "\n",
    "However, we need to be *very careful* here. Remember how an RNN performs sequence classification by only processing the *last* hidden state of a sequence? Now imagine that we were to pad sequence by adding $0$s at the end of a sequence: then the model would encounter a bunch of empty values in calculating the final hidden state, and wouldn't exactly learn much. So when implementing the `pad_batch` function below it is important that the padding happens on the left of the sequences. \n",
    "\n",
    "That is, the output of the padding should be a matrix with dimensions `(seq_len, batch_size)`, so with `seq_len` rows (the length of the longest sequence in the batch), and `batch_size` columns (the number of sequences in one batch). Visually this means that one item in the final `DataLoader` would look something like\n",
    "\n",
    "```\n",
    "            tensor([[   0,    0,    0,  ...,    0,    0,    0],\n",
    "                    [   0,    0,    0,  ...,    0,    0,    0],\n",
    "                    [   0,    0,    0,  ...,    0,    0,    0],\n",
    "                    ...,\n",
    "                    [1694,    4,  394,  ...,   48,   12,  189],\n",
    "                    [4568, 1564,   82,  ...,   45,    3,   69],\n",
    "                    [  37,   10, 2348,  ...,   24, 1572,  222]])\n",
    "```\n",
    "\n",
    "You are free to implement the function `my_sequence_pad` as you see fit. You could of course use the built-in PyTorch function `pad_sequence`, but this adds the trailing 0's at the end of a sequence, so you'd have to hack around that :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Solution:\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "def split_data(data, val_cutoff=0.8, test_cutoff=0.9):\n",
    "    random.Random(42).shuffle(data)\n",
    "    train = data[:int(val_cutoff*len(data))]\n",
    "    val = data[int(val_cutoff*len(data)):int(test_cutoff*len(data))]\n",
    "    test = data[int(test_cutoff*len(data)):]\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, X: List[torch.LongTensor], Y: List[torch.FloatTensor]) -> None:\n",
    "        super(ReviewDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.LongTensor, torch.FloatTensor]:\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "    \n",
    "def my_sequence_pad(sequences: List[LongTensor]) -> LongTensor:\n",
    "    NotImplemented    \n",
    "\n",
    "\n",
    "def pad_batch(batch: List[Tuple[LongTensor, FloatTensor]]) -> Tuple[LongTensor, FloatTensor]:\n",
    "    return my_sequence_pad([sample[0] for sample in batch]), torch.stack([sample[1] for sample in batch])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75186bbc",
   "metadata": {},
   "source": [
    "Again, you can verify that your implementation is correct by first running the cell directly below, and then the testing code below it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews_train, tokenized_reviews_val, tokenized_reviews_test = split_data(tokenized_reviews, val_cutoff=0.8, test_cutoff=0.9)\n",
    "sentiment_train, sentiment_val, sentiment_test = split_data(sentiment_tensors, val_cutoff=0.8, test_cutoff=0.9)\n",
    "\n",
    "tok_train_dataset = ReviewDataset(tokenized_reviews_train, sentiment_train)\n",
    "tok_train_dataloader = DataLoader(tok_train_dataset, collate_fn=pad_batch, shuffle=True, batch_size=32)\n",
    "\n",
    "tok_val_dataset = ReviewDataset(tokenized_reviews_val, sentiment_val)\n",
    "tok_val_dataloader = DataLoader(tok_val_dataset, collate_fn=pad_batch, shuffle=False, batch_size=32)\n",
    "\n",
    "tok_test_dataset = ReviewDataset(tokenized_reviews_test, sentiment_test)\n",
    "tok_test_dataloader = DataLoader(tok_test_dataset, collate_fn=pad_batch, shuffle=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b66630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training data\n",
    "for sx, sy in tok_train_dataloader:\n",
    "    display(sx.shape)\n",
    "    display(sx)\n",
    "    display(sy)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5bed8d",
   "metadata": {},
   "source": [
    "## Part 5 (30 points): Model training\n",
    "\n",
    "So far so good! You are now ready to define the model and the training loop. In this part you will implement the model, figure out the appropriate output activation and loss function, and see the model in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c988594e",
   "metadata": {},
   "source": [
    "### Part 5.1 (14 points): Defining the model\n",
    "\n",
    "Let's start with the model. Part of the implementation is already given: the `embedding` will hold the word embeddings, and as such will map from the `input_dim` (the vocabulary size) to the `embedding_dim`.\n",
    "\n",
    "After that, we want to process the embedded sequence with an LSTM layer, and use a Linear layer to process the final hidden state of the sequence. Finally, the output activation is applied to the output.\n",
    "\n",
    "Implement this model by finishing the code below. Note that the [LSTM layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) will automatically take care of initializing the hidden state vector of the 0th timestep of a sequence, so you don't have to do that yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a1c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Solution:\n",
    "\n",
    "class LSTMSequenceClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim: int, embedding_dim: int, hidden_dim: int, output_dim: int, output_activation):\n",
    "        super(LSTMSequenceClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = NotImplemented\n",
    "        self.h_to_y = NotImplemented\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text dim: [sentence length, batch size]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded dim: [sentence length, batch size, embedding dim]\n",
    "        \n",
    "        output = NotImplemented\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86476a9e",
   "metadata": {},
   "source": [
    "Great job! As a reward, here is some free code: it implements the training and evaluation loops, for computing the loss over the training and validation sets, and updating the parameters when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedf4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(network: torch.nn.Module,\n",
    "                X_batch: LongTensor,\n",
    "                Y_batch: FloatTensor,\n",
    "                loss_fn: Callable[[FloatTensor, FloatTensor], FloatTensor],  \n",
    "                optimizer: torch.optim.Optimizer) -> float:\n",
    "    network.train()\n",
    "    \n",
    "    prediction_batch = network(X_batch)  # forward pass\n",
    "    batch_loss = loss_fn(prediction_batch.squeeze(), Y_batch)  # loss calculation\n",
    "    batch_loss.backward()  # gradient computation\n",
    "    optimizer.step()  # back-propagation\n",
    "    optimizer.zero_grad()  # gradient reset\n",
    "    return batch_loss.item()\n",
    "\n",
    "\n",
    "def train_epoch(network: torch.nn.Module, \n",
    "                dataloader: DataLoader,\n",
    "                loss_fn: Callable[[FloatTensor, FloatTensor], FloatTensor],\n",
    "                optimizer: torch.optim.Optimizer, \n",
    "                device: str) -> float:\n",
    "    \n",
    "    loss = 0.\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(tqdm(dataloader)):\n",
    "        x_batch = x_batch.to(device)  # convert back to your chosen device\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss += train_batch(network=network, X_batch=x_batch, Y_batch=y_batch, \n",
    "                            loss_fn=loss_fn, optimizer=optimizer)\n",
    "    loss /= (i+1) # divide loss by number of batches for consistency \n",
    "        \n",
    "    return loss\n",
    "\n",
    "def eval_batch(network: torch.nn.Module,\n",
    "                X_batch: LongTensor,\n",
    "                Y_batch: FloatTensor,\n",
    "                loss_fn: Callable[[FloatTensor, FloatTensor], FloatTensor]) -> float:\n",
    "    network.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction_batch = network(X_batch)  # forward pass\n",
    "        batch_loss = loss_fn(prediction_batch.squeeze(), Y_batch)  # loss calculation\n",
    "        \n",
    "    return batch_loss.item()\n",
    "\n",
    "def eval_epoch(network: torch.nn.Module, \n",
    "                # a list of data points x\n",
    "                dataloader: DataLoader,\n",
    "                loss_fn: Callable[[FloatTensor, FloatTensor], FloatTensor],\n",
    "                device: str) -> float:\n",
    "    \n",
    "    loss = 0.\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss += eval_batch(network=network, X_batch=x_batch, Y_batch=y_batch, loss_fn=loss_fn)\n",
    "    loss /= (i+1)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e03d2",
   "metadata": {},
   "source": [
    "### Part 5.2 (6 points): Output activation, loss function\n",
    "\n",
    "You are almost ready to start training your model! There are just a few ingredients missing.\n",
    "\n",
    "First of all, instantiate your model below. Make sure that its `input_dim` has the length of the vocabulary, plus an extra dimension for the padding value. You can pick custom dimensions for the word embeddings and the hidden states. Given that the task is binary classification, define the appropriate output dimension and corresponding output activation function, passed as arguments to the model as well.\n",
    "\n",
    "Combine the model with the appropriate loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Solution:\n",
    "\n",
    "model = NotImplemented\n",
    "loss_fn = NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89105e91",
   "metadata": {},
   "source": [
    "### Part 5.3 (6 points): Evaluation metric\n",
    "\n",
    "The final thing is to compute the accuracy of the model: the number of correct classifications out the total number of items. Finish the implementation below to define an appropriate way of calculating accuracy, given your choices above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f910f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Solution:\n",
    "\n",
    "def measure_accuracy(network: torch.nn.Module,\n",
    "                     dataloader: DataLoader,\n",
    "                     device: str) -> float:\n",
    "    \"\"\"\n",
    "        Given a network, a dataloader and a device, iterates over the \n",
    "        dataset and returns the network's accuracy.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        pred = network(x_batch.to(device))\n",
    "        local_total = y_batch.shape[0]\n",
    "        local_correct = NotImplemented\n",
    "        correct += local_correct\n",
    "        total += local_total\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab93929",
   "metadata": {},
   "source": [
    "### Part 5.4 (4 points): Putting it all together, testing.\n",
    "\n",
    "You are so ready to run your model! Execute the code below to train your model for 5 epochs, and observe the validation accuracy, it should be well over 80%. Use the code after it to plot the training and validation loss, as well as the validation accuracies. Then, add your own code to report the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ba919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# you can set the device to 'cuda' if you have a GPU-enabled PyTorch installation,\n",
    "# or to 'mps' if you have an M1 Macbook.\n",
    "device = 'cpu'\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=0.001)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for t in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = train_epoch(model, tok_train_dataloader, optimizer=optimizer, loss_fn=loss_fn, device=device)\n",
    "    val_loss = eval_epoch(model, tok_val_dataloader, loss_fn, device=device)\n",
    "    val_acc = measure_accuracy(model, tok_val_dataloader, device=device)\n",
    "    \n",
    "    print('Epoch {}'.format(t))\n",
    "    print(' Training Loss: {}'.format(train_loss))\n",
    "    print(' Validation Loss: {}'.format(val_loss))\n",
    "    print(' Validation Accuracy: {}'.format(val_acc))\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8595b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for plotting\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.plot(val_accuracies)\n",
    "plt.legend(['Train Loss', 'Validation Loss', 'Validation Accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a5a7d",
   "metadata": {},
   "source": [
    "As the very last step of the model training phase, report the accuracy on the test portion of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3532454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Solution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9aafa5",
   "metadata": {},
   "source": [
    "### Part 5.5 (BONUS question): variation in your model\n",
    "\n",
    "If you feel up for it, experiment with a number of variations on the simple model you implemented.\n",
    "You can investigate the following suggestions:\n",
    "\n",
    "   1. Use a stacked LSTM, i.e. where one LSTM feeds into the next etc. Only if you have enough computational power!\n",
    "   2. Experiment with the dropout parameter (which randomly chooses to *not* update a certain percentage of parameters during training).\n",
    "   3. Use a bidirectional LSTM.\n",
    "   4. Replace the Linear output layer with multiple Linear layers.\n",
    "    \n",
    "You will receive bonus points for a decent attempt at improvement, as long as you reach equal or higher accuracy compared to the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e438147",
   "metadata": {},
   "source": [
    "## Part 6 (20 points): Analysis\n",
    "\n",
    "Now that you have a trained model that works reasonably well, we will investigate the embedding space it has created, along the different word usage in positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8061a4c",
   "metadata": {},
   "source": [
    "### Part 6.1 (3 points): Untokenization for data inspection\n",
    "\n",
    "First, let's take a look at the way words are distributed after tokenization among positive and negative reviews. To do so, we need to untokenize the tokenized reviews. To this end, finish the beneath implementation: it will create the inverted `int2word` map to map the tokenized integer representations of reviews back to the corresponding words, and the subsequent `untokenize` function will take a tokenized review (as a PyTorch Tensor) and produce a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b35031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Solution:\n",
    "int2word = NotImplemented\n",
    "\n",
    "def untokenize(review: Tensor) -> List[str]:\n",
    "    NotImplemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bad668",
   "metadata": {},
   "source": [
    "### Part 6.2 (5 points): Positive and Negative vocabularies\n",
    "\n",
    "Next, we want to look at words that are more specific to the positive and negative reviews. Given the positive and negative tokenized reviews, determine the word frequencies for all positive, and all negative reviews.\n",
    "Use this to compute the following three items:\n",
    "\n",
    "1. The words that were among the most common 20 words for the positive Ã¡nd negative reviews,\n",
    "2. The words that occurred more than 1500 times more often in the positive reviews, but not in the list of item '1',\n",
    "2. The words that occurred more than 2000 times more often in the negative reviews, but not in the list of item '1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556f723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Solution:\n",
    "positive_tokenized_reviews = [r for r,s in zip(tokenized_reviews, sentiment_tensors) if s.item() == 1.]\n",
    "negative_tokenized_reviews = [r for r,s in zip(tokenized_reviews, sentiment_tensors) if s.item() == 0.]\n",
    "\n",
    "\n",
    "NotImplemented\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dcb129",
   "metadata": {},
   "source": [
    "### Part 6.3 (6 points): Word similarities\n",
    "\n",
    "The model you trained has internally learnt to represent each word in the vocabulary by an embedding. It is interesting to see how these word embeddings function after learning to do review classification.\n",
    "\n",
    "We can use PyTorch's internal cosine similarity function to compute the *pairwise* cosine similarities between two matrices: given a matrix $M : W \\times D$ and $N: W \\times D$, with $W$ the number of words and $D$ the dimensions, the function `similarities` below returns a vector $S: W$ containing the pairwise cosine similarity between the words in $M$ and in $N$ (example: if $M[5]$ represents the word 'great' and $N[5]$ represents the word 'cool', then $S[5]$ holds the cosine similarity between them).\n",
    "\n",
    "So if we wanted to compute the similarity of one word against a list of other words, we need to stack the vector for the single word to get a matrix of the same size as the matrix containing the embeddings for the other words.\n",
    "\n",
    "The code below provides the functionality to compute pairwise similarity over two matrices, and a way to retrieve all the embeddings for a list of words. For ease of use the model is placed back on CPU. Run the two cells below to verify that the model contains 5000 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960a3652",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "def similarities(matrix1: Tensor, matrix2: Tensor):\n",
    "    return torch.nn.CosineSimilarity(dim=1)(matrix1, matrix2)\n",
    "\n",
    "def get_vectors(model: LSTMSequenceClassifier, words: List[str], word2int: dict[str, int]):\n",
    "    return model.embedding(torch.tensor([word2int[word] for word in words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ae301",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = get_vectors(model, list(word2int.keys()), word2int)\n",
    "display(all_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6ba594",
   "metadata": {},
   "source": [
    "Now it is your turn. Start by finishing the function `get_similarities_for_word` below. Given a single word and a list of words to compare against, it should return a tensor containing the similarities for the single word against all the other words. You can verify the implementation by running the test code afterwards, which will display the top 10 words for a few examples; of course, the word itself will occur as the most similar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Solution:\n",
    "\n",
    "def get_similarities_for_word(word: str, word_list: List[str]) -> Tensor:\n",
    "    NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b07497",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix1 = get_similarities_for_word('horrible', list(word2int.keys()))\n",
    "display([int2word[k.item()+1] for k in sim_matrix1.topk(10).indices])\n",
    "\n",
    "sim_matrix2 = get_similarities_for_word('awesome', list(word2int.keys()))\n",
    "display([int2word[k.item()+1] for k in sim_matrix2.topk(10).indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a48817",
   "metadata": {},
   "source": [
    "### Part 6.4 (6 points): Closeness of the embedding space\n",
    "\n",
    "As the very final part, we wish to inspect the average similarity between word lists. Implement the function `get_average_similarities` below, which takes in two lists of words and returns the arithmetic mean of the similarities between them. You should make use of your prior implementation that retrieves similarities for one word against a list of words. Given that you computed overlapping common words, distinctly positive words, and distinctly negative words in 3.2, use your implemtation to calculate the following average similarities:\n",
    "\n",
    " 1. Between the distinctly positive words and the overlapping common words,\n",
    " 2. Between the distinctly negative words and the overlapping common words,\n",
    " 3. Between the distinctly positive words and the distinctly negative word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da902624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 Solution:\n",
    "\n",
    "def get_average_similarities(word_list1: List[str], word_list2: List[str]) -> float:\n",
    "    NotImplemented"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
